> çˆ¬è™«å¿ƒäº‹123ï¼šä»åˆšè®¤è¯†Pythonåˆ°ç°åœ¨ï¼Œç›®å…‰æ‰€åŠä¹‹å¤„å¿…æœ‰çˆ¬è™«æ–‡ç« ã€‚æˆ‘å´ä¸€ç›´ä¸æ„Ÿå…´è¶£ï¼Œä¸€æ˜¯ç½‘ä¸Šçˆ¬å–æ•°æ®å¹¶åˆ†æçš„æ–‡ç« ä¸å…‰å±•ç°äº†æŠ€æœ¯å®åŠ›ï¼Œè¿˜æœ‰ä½œè€…æ€è·¯æ¸…æ™°åˆ†æçš„å¤´å¤´æ˜¯é“ï¼Œä½œæ–‡æ°´å“ä¹Ÿä½¿æˆ‘æƒ­æ„§ï¼Œæ‰€ä»¥æˆ‘ä¸æ„¿æ²¦ä¸ºé»‘å®¢ç››è¡Œæ—¶ä»£çš„è„šæœ¬å°å­ä¹‹æµã€‚äºŒæ˜¯ä»¥å‰å…¬å¸éœ€è¦æ–°é—»é‡‡é›†ï¼Œé‚£æ—¶ä¸æ‡‚çˆ¬è™«ä¹‹ç±»ï¼Œä¾¿ç”¨PHPå†™äº†é‡‡é›†æ–°é—»çš„é¡µé¢ï¼Œå¡«å†™URLï¼Œå·¦å³æ ‡ç­¾è¿‡æ»¤ä¹‹åï¼Œæ‹¿æ–°é—»åˆ—è¡¨å’Œæ–°é—»ã€‚çŸ¥é“æœ‰çˆ¬è™«ä¹‹åï¼Œå°±è®¤ä¸ºæˆ‘ç”¨PHPå†™çš„ä¹Ÿç®—æœ€ç®€é™‹çš„çˆ¬è™«ï¼Œä¾¿å¯¹çˆ¬è™«æ²¡æœ‰å¤šå¤§çš„å…´è¶£ã€‚
>
> å°±åƒåˆä¸­å‘¨è‘£ç«çš„ä¸€å¡Œç³Šæ¶‚ï¼Œæˆ‘å´ä¸æ„¿æ„å¬ï¼Œé™¤äº†èµ„æºéš¾è·å–å¤–ä¸»è¦è¿˜æ˜¯æŠ—æ‹’å¤§å®¶éƒ½åœ¨ç‹‚çƒ­çš„ä¸œè¥¿å°±æƒ³åšä¸ä¸€æ ·çš„äººã€‚ç„¶è€Œï¼Œç°åœ¨è¿˜èƒ½æƒ³èµ·æœ‰ä¸€å¹´çš„æš‘å‡ä½œä¸šæœ€åé¢å¸¦æœ‰ä¸ƒé‡Œé¦™çš„æ­Œè¯ **çª—å¤–çš„éº»é›€ åœ¨ç”µçº¿æ†ä¸Šå¤šå˜´  ä½ è¯´è¿™ä¸€å¥ å¾ˆæœ‰å¤å¤©çš„æ„Ÿè§‰**ï¼Œå·å·å¬ç€å‘¨è‘£è¡¥é’æ˜¥ã€‚ 

### 1. hello world ä¸ Requests åº“

> Requests å”¯ä¸€çš„ä¸€ä¸ª**éè½¬åŸºå› **çš„ Python HTTP åº“ï¼Œäººç±»å¯ä»¥å®‰å…¨äº«ç”¨ã€‚ 

**çˆ¬å–ç™¾åº¦é¦–é¡µ**

```python
# coding:utf-8
import requests

resp = requests.get('https://www.baidu.com')
html = resp.text.encode('ISO-8859-1').decode('utf-8')
print(resp.encoding)
print(html)
```
åˆšå¼€å§‹ä¸­æ–‡å­—ä¸ºä¹±ç ï¼Œä»¥ä¸ºæ˜¯gbkä¾¿è½¬äº†ä¸€æ¬¡ï¼Œè¿˜æ˜¯ä¸è¡Œï¼Œç›´åˆ°åœ¨[pythonçˆ¬è™«ç¼–ç å½»åº•è§£å†³](https://blog.csdn.net/qq_36278071/article/details/79660196)ä¸­çŸ¥é“`requests.Response `ç±»å‹çš„ `encoding`å±æ€§å¯ä»¥å¾—åˆ°ç¼–ç ï¼Œè¾“å‡ºæ˜¯`ISO-8859-1`ç¼–ç ã€‚`chardet `ä¹‹æµçš„è¿˜æ˜¯ä¸ç”¨äº†ã€‚

å¦‚æœæŠ¥`SSL`é”™è¯¯ï¼Œ`resp = requests.get('https://www.baidu.com', verify=False) `æ·»åŠ `verify=False`å¿½ç•¥è¯ä¹¦å³å¯ã€‚

**å›¾ç‰‡äºŒè¿›åˆ¶æ–‡ä»¶è·å–åŠæ³•**

```python
# coding:utf-8
import requests
import os
import sys

url = 'http://docs.python-requests.org/zh_CN/latest/_static/requests-sidebar.png'
resp = requests.get(url)

img_name = os.path.basename(url)
img_path = sys.path[0]+'/'+img_name
# print(resp.content)
with open(img_path, 'wb') as f:
    f.write(resp.content)
    print(f"download image file : {img_path}")
```

å°†äºŒè¿›åˆ¶å†™å…¥æ–‡ä»¶ã€‚

**response.content å’Œ response.text åŒºåˆ«**

ä¸Šé¢ä¸¤æ¬¡åˆ†åˆ«ç”¨äº†`resp.text`å’Œ`resp.content`è·å–å“åº”æ•°æ®ï¼Œé‚£ä¹ˆåŒºåˆ«åœ¨å“ªï¼Ÿ

`response.content`è·å–çš„æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œ`resp.text`è·å–æ–‡æœ¬ã€‚

[python response.text å’Œresponse.contentçš„åŒºåˆ«](https://blog.csdn.net/lbj1260200629/article/details/82997524) ä¸­æ‘˜å½•ï¼š

*response.content*

```
- ç±»å‹ï¼šbytes
- è§£ç ç±»å‹ï¼š æ²¡æœ‰æŒ‡å®š
- å¦‚ä½•ä¿®æ”¹ç¼–ç æ–¹å¼ï¼šresponse.content.deocde("utf-8")
```

*response.text*

```
- ç±»å‹ï¼šstr
- è§£ç ç±»å‹ï¼š æ ¹æ®HTTP å¤´éƒ¨å¯¹å“åº”çš„ç¼–ç ä½œå‡ºæœ‰æ ¹æ®çš„æ¨æµ‹ï¼Œæ¨æµ‹çš„æ–‡æœ¬ç¼–ç 
- å¦‚ä½•ä¿®æ”¹ç¼–ç æ–¹å¼ï¼šresponse.encoding="gbk"
```

**å‹¤æŸ¥æ‰‹å†Œ**

é‚£ä¹ˆresponseé™¤äº†ä¸Šé¢ç”¨è¿‡çš„`encoding` `content` `text` å¤–è¿˜æœ‰ä»€ä¹ˆå±æ€§ï¼Ÿå»å®˜ç½‘å…¥é—¨èƒ½æ‰‹å†Œçœ‹çœ‹ [Requests å¿«é€Ÿä¸Šæ‰‹](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)ï¼Œå¾ˆçŸ­å¾ˆç®€æ´å¾ˆæ¸…æ™°ã€‚

ä»é‡Œé¢äº†è§£åˆ°ï¼š

> ä½ å¯ä»¥æ‰¾å‡º Requests ä½¿ç”¨äº†ä»€ä¹ˆç¼–ç ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨`r.encoding` å±æ€§æ¥æ”¹å˜å®ƒï¼š

```
>>> r.encoding
'utf-8'
>>> r.encoding = 'ISO-8859-1'
```

> **å¦‚æœä½ æ”¹å˜äº†ç¼–ç ï¼Œæ¯å½“ä½ è®¿é—® `r.text` ï¼ŒRequest éƒ½å°†ä¼šä½¿ç”¨ `r.encoding` çš„æ–°å€¼**ã€‚

æ‰€ä»¥ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼Œè·å–ç™¾åº¦é¦–é¡µä¸­`html = resp.text.encode('ISO-8859-1').decode('utf-8')`ç”¨ISO-8859-1ç¼–ç å†è§£ç æ˜¯ä¸æ˜¯æœ‰ç‚¹å¤šä½™ï¼Œè¯•ç€æŒ‡å®š`encoding`åå†è·å–å†…å®¹ã€‚

```python
resp = requests.get('https://www.baidu.com', verify=False)
# html = resp.text.encode('ISO-8859-1').decode('utf-8')
# print(resp.encoding)
# print(html)
resp.encoding = 'utf-8'
print(resp.text)
```

å¯ä»¥è·å–ï¼æ‰€ä»¥æœ‰ä¸€ä»½æ¸…æ™°çš„æ‰‹å†Œæ˜¯å¤šä¹ˆçš„æœ‰ç”¨ã€‚æ„Ÿè°¢`Requests`å„ä½ä½œè€… â¤

### 2. Beautiful Soup åº“ æå–æ•°æ®

> [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) æ˜¯ä¸€ä¸ªå¯ä»¥ä»HTMLæˆ–XMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„Pythonåº“.

```python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

soup = BeautifulSoup(html_doc, 'lxml')
# print(soup.prettify())
# print(soup.get_text())

# è·å–æ ‡é¢˜æ ‡ç­¾ã€æ ‡ç­¾åã€æ ‡é¢˜å†…å®¹
title_tag = soup.title
tag_name = soup.title.name
title = soup.title.string
print(title_tag)
print(tag_name)
print(title)

print(soup.find_all('a'))
```

å› ä¸ºæ²¡æœ‰æŒ‡å®šè§£æå™¨ï¼Œç³»ç»Ÿä¸­å®‰è£…äº†`lxml`ï¼Œæ‰€ä»¥æœ‰è­¦å‘Šè¯´é»˜è®¤ä½¿ç”¨*ç³»ç»Ÿä¸­å·²æœ‰æœ€ä½³å¯ç”¨çš„HTMLè§£æå™¨*  `lxml`ï¼Œä½†æ˜¯å› ä¸ºå¯èƒ½å…¶ä»–æœºå™¨æ²¡æœ‰ï¼Œæ‰€ä»¥è¦æ³¨æ„ç§»æ¤æ€§å•¦ã€‚

> UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). 

> Beautiful Soupé€‰æ‹©æœ€åˆé€‚çš„è§£æå™¨æ¥è§£æè¿™æ®µæ–‡æ¡£,å¦‚æœæ‰‹åŠ¨æŒ‡å®šè§£æå™¨é‚£ä¹ˆBeautiful Soupä¼šé€‰æ‹©æŒ‡å®šçš„è§£æå™¨æ¥è§£ææ–‡æ¡£. 

è·å–æ ‡ç­¾å’Œæ–‡æœ¬ï¼š

```python
from bs4 import BeautifulSoup

lists = """
<a class="a1" href="http://www.baidu.com/">ç™¾åº¦</a>, 
<a class="a2" href="http://www.163.com/">ç½‘æ˜“</a>, 
<a class="a3" href="http://www.sina.com/">æ–°æµª</a>
"""
b = BeautifulSoup(lists, 'lxml')
a_list = b.find_all('a')

print(a_list)
for a in a_list:
    print(a.get_text())  # get_text è·å–æ ‡ç­¾æ–‡æœ¬
    print(a.get('href'))  # get è·å–æ ‡ç­¾å±æ€§
    print(a['href'])  # åŒä¸Š
```

è·å–æ•™ç¨‹é¡µé¢çš„Top5 `<code>`å…ƒç´ (mdä¸­çš„ **`**):

```python
# coding:utf-8
import requests
from bs4 import BeautifulSoup
from collections import Counter


def get_count(data):
    """ è·å–å…ƒç´ å‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰5å """
    count = Counter(data)
    count = count.most_common(5)
    return dict(count)


html = requests.get('https://www.yukunweb.com/2017/6/python-spider-BeautifulSoup-basic/')
content = html.text

soup = BeautifulSoup(content, 'lxml')
codes = soup.find_all('code')
top5 = get_count(codes)
print(top5)
```

é¡µé¢å‡ºç°Top5çš„codeå…ƒç´ å’Œæ¬¡æ•°ï¼š

{`BeautifulSoup`: 12, `find_all()`: 12, `lxml`: 9, `Python`: 8, `find()`: 6}

æ³¨ï¼šcssé€‰æ‹©å™¨`select()`å¾ˆå¼ºå¤§ï¼Œä¸€èˆ¬éƒ½å¯ä»¥åŒ¹é…åˆ°ï¼Œè‹¥ä¸ç†Ÿæ‚‰ï¼Œå¯ä½¿ç”¨Chromeå°åŠŸèƒ½ï¼šå…ƒç´ å³å‡»->copy->Copy selector

![select.png](http://image.acfuu.com/mdImages/201812/select.png)

æ‹¿åˆ° `#\31 763845315 > div.reply-doc.content > p`ï¼Œåªç”¨`div.reply-doc.content > p`å³å¯ã€‚

ç”¨çš„æ—¶å€™è¿˜æ˜¯å¤š[Beautiful Soup 4.2.0 æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)ï¼Œæœ‰ä¸€ç‚¹ç‚¹é•¿ï¼ŒæŠ½æ—¶é—´çœ‹çœ‹å°±ä¼šå¾—å¿ƒåº”æ‰‹ã€‚

#### å®æˆ˜ï¼šä¸‹è½½çº¢æ¥¼æ¢¦

æƒ³è¦ä¸‹è½½çº¢æ¥¼ï¼Œæ‰¾åˆ°ä¸€ä¸ª[çº¢æ¥¼æ¢¦èµ„æº](https://m.2011mv.com/res/6154/)ï¼ŒæŸ¥çœ‹æºç ï¼š

![hl_resources.png](http://image.acfuu.com/mdImages/201812/hl_resources.png)

åˆ—è¡¨åœ¨`download_list`ä¸‹ï¼Œ`download_title`ä¸‹çš„`a`æ ‡ç­¾ä¸ºæ¯æ¡çš„é“¾æ¥ã€‚ç”¨`soup.select('.download_list .download_title > a')`è·å–æ¯æ¡é“¾æ¥ã€‚å°±å¯ä»¥æ‹¿åˆ°æ¯æ¡`ed2kç£åŠ›é“¾`

```
çº¢æ¥¼æ¢¦.02.å®é»›é’—åˆä¼šè£åº†å ‚.mkv ed2k://|file|%E7%BA%A2%E6%A5%BC%E6%A2%A6.02.%E5%AE%9D%E9%BB%9B%E9%92%97%E5%88%9D%E4%BC%9A%E8%8D%A3%E5%BA%86%E5%A0%82.mkv|922919381|4458969531F1D4153EAB37F1E80F4AC2|/
çº¢æ¥¼æ¢¦.23.æ…§ç´«å¨Ÿæƒ…è¾è¯•å¿™ç‰.mkv ed2k://|file|%E7%BA%A2%E6%A5%BC%E6%A2%A6.23.%E6%85%A7%E7%B4%AB%E5%A8%9F%E6%83%85%E8%BE%9E%E8%AF%95%E5%BF%99%E7%8E%89.mkv|907433268|2AFA1AE36A5BC34F76CF07621B0D00F5|/
â€¦â€¦
```

åªæ‰“å°é“¾æ¥ç„¶åå¤åˆ¶åˆ°è¿…é›·ä¹Ÿå¯ä»¥ï¼Œä½†æ˜¯æ‰¾åˆ°ä¸€ä¸ªè¿…é›·ä¸‹è½½

```python
# coding:utf-8
""" è°ƒç”¨è¿…é›·ä¸‹è½½ """
import subprocess
import base64
thunder_path = r'C:\Program Files (x86)\Thunder Network\Thunder\Program\Thunder.exe'


def Url2Thunder(url):
    url = 'AA' + url + 'ZZ'
    url = base64.b64encode(url.encode('ascii'))
    url = b'thunder://' + url
    thunder_url = url.decode()
    return thunder_url


def download_with_thunder(file_url):
    thunder_url = Url2Thunder(file_url)
    subprocess.call([thunder_path, thunder_url])
```

æ‹¿æ¥é›†æˆä¸‹è½½ä¹‹åï¼Œç›´æ¥æ‰§è¡Œå°±èƒ½çœ‹åˆ°è°ƒç”¨ä¸‹è½½åˆ—è¡¨äº†ï¼š

>  æ³¨ï¼šed2kä¸éœ€è¦é€šè¿‡`Url2Thunder(url)`è½¬æˆ`thunder`åœ°å€ï¼Œä½†æ˜¯è½¬äº†ä¹Ÿä¸å½±å“ã€‚

![thunder_hl.png](http://image.acfuu.com/mdImages/201812/thunder_hl.png)

ç®€å•å°è£…ä¸€æ¬¡ä½¿è°ƒç”¨æ›´ç®€å•ï¼Œè€Œä¸”è¦å¯é™åˆ¶æ¡æ•°ï¼Œåƒlimitï¼Œç”¨åˆ‡ç‰‡å®Œæˆã€‚æ¯”å¦‚ç¬¬äºŒéƒ¨æˆ‘æƒ³ä¸‹è½½é£éªšå¾‹å¸ˆï¼Œä½†æ˜¯èµ„æºé‡Œé¢æœ‰4å­£ï¼Œè€Œæˆ‘åªæƒ³è¦ç¬¬ä¸‰å­£çš„åé›†ã€‚

![fslo.png](http://image.acfuu.com/mdImages/201812/fslo.png)

```python
if __name__ == '__main__':
    # çº¢æ¥¼
    hl_down = Download(file_name='hl.html', encoding='GB2312')  # ç¬¬äºŒæ¬¡å¯åŠ debug=True, ç”¨æ–‡ä»¶è°ƒè¯•ï¼Œé¿å…ç›´æ¥è¯·æ±‚
    hl_down.get_video(
        'https://m.2011mv.com/res/6154/',
        '.download_list .download_title > a',
        'ed2k')

    # é£éªšå¾‹å¸ˆ ç¬¬ä¸‰å­£
    hl_down = Download(file_name='fxlo3.html', encoding='GB2312')
    hl_down.get_video(
        'https://m.2011mv.com/res/13969/',
        '.introtext table a',
        'href',
        10, 20)
```

![fslo3.png](http://image.acfuu.com/mdImages/201812/fslo3.png)

å¯ä»¥çœ‹åˆ°åˆšå¥½æŠ“å–çš„æ˜¯ç¬¬ä¸‰å­£åé›†ï¼Œä¸è¿‡èµ„æºæœ¬èº«æœ‰é—®é¢˜æœ€åä¸èƒ½ä¸‹è½½ï¼Œå¯¹äºè¿™ç¯‡æœ¬èº«æ²¡æœ‰å½±å“ã€‚

### 3. æŠ“å–åœŸå‘³æƒ…è¯ï¼ˆæ­£åˆ™å’Œbs4åˆ†åˆ«è§£æé¡µé¢ï¼‰

æ€ä¹ˆåœ¨è±†ç“£è´´å›å¤ä¸­æ‰¾åˆ°éœ€è¦çš„åœŸå‘³æƒ…è¯ï¼Ÿå–å­—æ•°ï¼Œä¸å¦¥ï¼Œè¯•è¯•æƒ…æ„Ÿåˆ†æå§ã€‚ç”¨**`snownlp`åº“**ã€‚

è·å–å½“å‰é¡µæ‰€æœ‰è¯„è®ºï¼š

```python
soup = BeautifulSoup(html, 'lxml')
content = soup.select('#comments div.reply-doc.content > p')
```

åŠ äº†`#comments`idç­›é€‰è¯„è®ºï¼Œæ’é™¤é«˜èµçš„é‡å¤æ•°æ®ã€‚

ç»“æœï¼š

```
å¯æˆ‘æƒ³å’Œä½ ç»“å°¾
å–œæ¬¢æˆ‘å—ï¼Œå–œæ¬¢æˆ‘å°±å‘è±†é‚®ç»™æˆ‘ (ËŠoÌ´Ì¶Ì¤âŒ„oÌ´Ì¶Ì¤Ë‹)
æ’©
æ¥¼ä¸‹ç»§ç»­
æˆ‘æœ‰ä¸¤æŠŠæªï¼Œä¸€æŠŠå«å°„ï¼Œå¦ä¸€æŠŠå«å•Šï¼Œç¾æäº†!
æ¥¼ä¸‹æ¥
æˆ‘æ˜¯çµå„¿ä½ æ˜¯ä»€ä¹ˆå‘€
æˆ‘ä¸çŸ¥é“  å“ˆå“ˆå“ˆ
ä½ æ˜¯å®å½“å‘€
æ¥¼ä¸‹ç»§ç»­
è¢«ä½ ç‚¹èµçš„æœ‹å‹åœˆæ˜¯ç”œç”œåœˆğŸ©
å¥½ç”œ  æ¥¼ä¸‹ç»§ç»­
ä½ å–œæ¬¢å–æ°´å— å–œæ¬¢ æ­å–œä½ å·²ç»å–œæ¬¢70%çš„æˆ‘äº†
ç”œçš„  å°å¯çˆ±  æ¥¼ä¸‹ç»§ç»­
æœ€æœ‰è¶£çš„ç­‰å¾…æ˜¯æœ‰ä½ çš„æœªæ¥
â€¦â€¦
```

15æ¡ä¸­ï¼Œç›®æ ‡æ˜¯æœ€ç¬¦åˆçš„11 13 15, 1 5 ä¹Ÿè¾ƒç¬¦åˆï¼Œå…¶ä»–çš„å›å¤ä»¥åŠä¸çŸ¥æ‰€äº‘ä¹‹ç±»çš„å°±å¾—æŠ›å¼ƒäº†ã€‚

**åˆ†è¯ï¼Œjieba å’Œ snownlpè‡ªå¸¦**

```python
# åˆ†è¯
    for i in content:
        text = i.get_text()
        print(text)

        s = SnowNLP(text)
        j = jieba.cut(text, cut_all=False)
        print("-> jieba: {0}  \n snownlp: {1}".format(','.join(j), ','.join(s.words)))
```

é€‰å‡ å¤„ä¸ä¸€æ ·çš„åˆ†è¯ç»“æœ

```
è¢«ä½ ç‚¹èµçš„æœ‹å‹åœˆæ˜¯ç”œç”œåœˆğŸ©
-> jieba: è¢«,ä½ ,ç‚¹èµ,çš„,æœ‹å‹åœˆ,æ˜¯,ç”œç”œ,åœˆ,ğŸ©  
 snownlp: è¢«,ä½ ç‚¹,èµ,çš„,æœ‹å‹,åœˆ,æ˜¯,ç”œ,ç”œåœˆ,ğŸ©
å¥½ç”œ  æ¥¼ä¸‹ç»§ç»­
-> jieba: å¥½,ç”œ, , ,æ¥¼ä¸‹,ç»§ç»­  
 snownlp: å¥½,ç”œ,æ¥¼ä¸‹,ç»§ç»­
ä½ å–œæ¬¢å–æ°´å— å–œæ¬¢ æ­å–œä½ å·²ç»å–œæ¬¢70%çš„æˆ‘äº†
-> jieba: ä½ ,å–œæ¬¢,å–æ°´,å—, ,å–œæ¬¢, ,æ­å–œ,ä½ ,å·²ç»,å–œæ¬¢,70%,çš„,æˆ‘,äº†  
 snownlp: ä½ ,å–œæ¬¢,å–,æ°´,å—,å–œæ¬¢,æ­å–œ,ä½ ,å·²ç»,å–œæ¬¢,70%,çš„,æˆ‘,äº†
ç”œçš„  å°å¯çˆ±  æ¥¼ä¸‹ç»§ç»­
-> jieba: ç”œ,çš„, , ,å°å¯çˆ±, , ,æ¥¼ä¸‹,ç»§ç»­  
 snownlp: ç”œ,çš„,å°,å¯çˆ±,æ¥¼ä¸‹,ç»§ç»­
```

æ€»ä½“ä¸Šï¼Œè®ºåˆ†è¯è¿˜æ˜¯jiebaåˆ†è¯æ¯”è¾ƒä¸“ä¸šã€‚

**snownlp ç›´æ¥åˆ†æçš„ç»“æœ**

```python
for i in content:
        text = i.get_text()
        print(text)

        s = SnowNLP(text)
        print("snownlp: {0} ({1})".format(','.join(s.words), s.sentiments ))
```

æŒ‘é€‰æ¯”è¾ƒæ˜æ˜¾çš„ä¸€ä¾‹ï¼š

```
è¶…å–œæ¬¢è¿™å¥è¯è€¶ (0.7135350139260407)
è¿‘è§†600åº¦ï¼Œè€Œä½ æ˜¯æˆ‘å”¯ä¸€ä¸€ä¸ªä¸ç”¨æˆ´çœ¼é•œä¹Ÿèƒ½åœ¨200ç±³å¼€å¤–å°±ä¸€çœ¼è®¤å‡ºçš„å¥³å­©ğŸ‘§ğŸ» (0.28457337830335616)
```

è¿™æ ·ä¸è¡Œâ€¦â€¦åˆè¿‡äº†ä¸‰ä¸ªå°æ—¶ï¼Œ**å¼„ä¸äº†ï¼Œæ”¾å¼ƒï¼**

ç›´æ¥æ‰¾äº›åœŸå‘³æƒ…è¯åˆ—è¡¨ï¼Œçˆ¬å§ã€‚

**çˆ¬å–åœŸå‘³æƒ…è¯**

æ‰¾äº†ä¸¤ä¸ªé“¾æ¥åˆ†åˆ«æŠ“å–ï¼Œä½¿ç”¨ç®€å•å°è£…çš„`Download`æ¨¡å—ï¼Œç¬¬ä¸€æ¬¡æŠ“å–åˆ°ç½‘é¡µåç›´æ¥ç”¨æ­£åˆ™è·å–å†…å®¹ï¼Œç¬¬äºŒæ¬¡ç”¨`soup`é€‰æ‹©å™¨é€‰å–åï¼Œç”¨æ­£åˆ™æ‰¹é‡è·å–ç»“æœï¼Œæœ€åå­˜å…¥æ–‡æœ¬æ–‡ä»¶ã€‚æ¯æ¬¡ä½¿ç”¨çš„æ—¶å€™ï¼Œç¬¬ä¸€æ¬¡ä¸å¼€å¯`debug`ï¼Œå°†ä¼šè‡ªåŠ¨ä¿å­˜`html`æ–‡ä»¶ï¼Œä¹‹åå¼€å¯`debug`å¯ç”¨æ–‡ä»¶åˆ†æä¸ç”¨æ¯æ¬¡éƒ½æ‹‰ç½‘é¡µã€‚

```python
	# 1 https://www.guaze.com/juzi/16832.html è¿™ä¸ªæ•°æ®ç‰¹æ®Šï¼Œä¿å­˜ç”¨ISO 8859-1, æ‰“å¼€ç”¨GB2312
    # ä½¿ç”¨ï¼šç¬¬ä¸€æ¬¡ä¸å¼€å¯debugï¼Œå°†ä¼šå†™å…¥æ–‡ä»¶ï¼Œä¹‹åå¼€å¯debugå¯ç”¨æ–‡ä»¶åˆ†æã€‚
    # down = Download(file_name=sys.path[0]+'/whisper1.html', encoding='ISO 8859-1')
    down = Download(file_name=sys.path[0]+'/whisper1.html', encoding='GB2312', debug=True)  # ç¬¬äºŒæ¬¡å¯åŠ debug=True, ç”¨æ–‡ä»¶è°ƒè¯•ï¼Œé¿å…ç›´æ¥è¯·æ±‚
    html = down.get_text('https://www.guaze.com/juzi/16832.html', re.compile(r'\<p\>(\d+)[ã€]?((.*)(?=www)|(.*)(<\/p>))', re.I))
    # å¤„ç†æ•°æ®
    html = [i[3] for i in html]
    # å†™å…¥æ–‡ä»¶
    down.save(sys.path[0]+"/whispers.txt", html)

    # 2 https://baijiahao.baidu.com/s?id=1612847567100515849&wfr=spider&for=pc
    # down = Download(file_name=sys.path[0]+'/whisper2.html')
    down = Download(file_name=sys.path[0]+'/whisper2.html', debug=True)
    html = down.get_text('https://baijiahao.baidu.com/s?id=1612847567100515849&wfr=spider&for=pc', '.bjh-p')
    # å¤„ç†æ•°æ®
    html = '\n'.join([i.get_text() for i in html])
    html = re.findall(r'\d+[ã€](.*)', html)
    # å†™å…¥æ–‡ä»¶
    down.save(sys.path[0]+"/whispers.txt", html)
```

### 4. è±†ç“£2018å›¾ä¹¦æ¦œå• ï¼ˆjsonæ•°æ®è§£æï¼Œå›¾ç‰‡å­˜å‚¨ï¼‰

æœ€ç»ˆæ˜¯ï¼šçˆ¬å–æ‰€æœ‰åˆ†ç±»æ¦œï¼Œå¹¶å»ºç«‹æ–‡ä»¶å¤¹å­˜æ”¾æ¦œå•ä¹¦ç±å›¾ç‰‡ï¼Œæ•°æ®å†™å…¥sqliteã€‚

```
INFO:root:é¡µç ï¼š1...
INFO:root:åˆ›å»ºæ–‡ä»¶å¤¹
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\å¤±è¸ªçš„å­©å­.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\è¿½å¯»é€å»çš„æ—¶å…‰Â·ç¬¬ä¸€å·ï¼šå»æ–¯ä¸‡å®¶é‚£è¾¹.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\æˆ¿æ€çªçš„åˆæ‹ä¹å›­.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\å¥¥å¤æ–¯éƒ½.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\æˆ‘ä»¬ä¸€æ— æ‰€æœ‰.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\è«æ–¯ç§‘ç»…å£«.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\å¦‚çˆ¶å¦‚å­.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\è§‚å±±æµ·.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\æ¼«é•¿çš„å‘Šåˆ«.jpg saved.
INFO:root:f:\py\py-snippet\spider/data\2018å¹´åº¦é«˜åˆ†å›¾ä¹¦\å›ç­”ä¸äº†.jpg saved.
INFO:root:å†™å…¥åˆ†ç±»ï¼š[2018å¹´åº¦é«˜åˆ†å›¾ä¹¦], æ¡æ•°ï¼š10
INFO:root:é¡µç ï¼š2...
```

![douban_2018_books.png](http://image.acfuu.com/mdImages/201812/douban_2018_books.png)

æƒ³ç›´æ¥åœ¨shellé‡Œè¿è¡Œ`sqlite`å‘½ä»¤ï¼Œå¯ä»¥ç”¨å¦‚ä¸‹è„šæœ¬ï¼š
```python
# A minimal SQLite shell for experiments

import sqlite3

con = sqlite3.connect(":memory:")
con.isolation_level = None
cur = con.cursor()

buffer = ""

print("Enter your SQL commands to execute in sqlite3.")
print("Enter a blank line to exit.")

while True:
    line = input()
    if line == "":
        break
    buffer += line
    if sqlite3.complete_statement(buffer):
        try:
            buffer = buffer.strip()
            cur.execute(buffer)

            if buffer.lstrip().upper().startswith("SELECT"):
                print(cur.fetchall())
        except sqlite3.Error as e:
            print("An error occurred:", e.args[0])
        buffer = ""

con.close()
```
æŸ¥è¯¢æ•°æ®åº“ä¸­æ‰€æœ‰ä¹¦ç±å’Œå„åˆ†ç±»æ¦œä¹¦ç±æ•°ç›®ï¼š
```
Enter your SQL commands to execute in sqlite3.
Enter a blank line to exit.
select type, count(*) count from books;
[('book', 240)]
select class, count(*) count from books group by class order by id;
[('2018å¹´åº¦é«˜åˆ†å›¾ä¹¦', 10), ('2018å¹´åº¦æœ€å—å…³æ³¨å›¾ä¹¦', 10), ('2018å¹´åº¦å¤–å›½æ–‡å­¦ï¼ˆå°è¯´ç±»ï¼‰', 10), ('2018å¹´åº¦å¤–å›½æ–‡å­¦ï¼ˆéå°è¯´ç±»ï¼‰', 10), ('2018å¹´åº¦ä¸­å›½æ–‡å­¦ï¼ˆå°è¯´ç±»
ï¼‰', 10), ('2018å¹´åº¦ä¸­å›½æ–‡å­¦ï¼ˆéå°è¯´ç±»ï¼‰', 10), ('2018å¹´åº¦å†å²Â·æ–‡åŒ–', 10), ('2018å¹´åº¦ç¤¾ç§‘Â·çºªå®', 10), ('2018å¹´åº¦ä¼ è®°Â·äººç‰©', 10), ('2018å¹´åº¦ç§‘å­¦Â·æ–°çŸ¥', 10)
, ('2018å¹´åº¦è‰ºæœ¯Â·è®¾è®¡', 10), ('2018å¹´åº¦å½±è§†Â·æˆå‰§', 10), ('2018å¹´åº¦å•†ä¸šÂ·ç»ç®¡', 10), ('2018å¹´åº¦æ¸©æš–Â·æ²»æ„ˆ', 10), ('2018å¹´åº¦ç§‘å¹»Â·å¥‡å¹»', 10), ('2018å¹´åº¦æ‚¬ç–‘Â·
æ¨ç†', 10), ('2018å¹´åº¦ç»˜æœ¬Â·æ¼«ç”»', 10), ('2018å¹´åº¦å†ç‰ˆå¥½ä¹¦', 10), ('1æœˆçƒ­é—¨å›¾ä¹¦', 5), ('2æœˆçƒ­é—¨å›¾ä¹¦', 5), ('3æœˆçƒ­é—¨å›¾ä¹¦', 5), ('4æœˆçƒ­é—¨å›¾ä¹¦', 5), ('5æœˆçƒ­é—¨å›¾
ä¹¦', 5), ('6æœˆçƒ­é—¨å›¾ä¹¦', 5), ('7æœˆçƒ­é—¨å›¾ä¹¦', 5), ('8æœˆçƒ­é—¨å›¾ä¹¦', 5), ('9æœˆçƒ­é—¨å›¾ä¹¦', 5), ('10æœˆçƒ­é—¨å›¾ä¹¦', 5), ('11æœˆçƒ­é—¨å›¾ä¹¦', 5), ('12æœˆçƒ­é—¨å›¾ä¹¦', 5)]

```

**æ’æ›²ï¼š**

éœ€è¦ç”Ÿæˆå ä½ç¬¦æ—¶ç”¨`occupy = tuple(['?']*len(keys))` å‘ç°å…¨æ˜¯`'?'`ï¼Œåº”è¯¥åªè¦`?`

```sql
insert into books('class', 'title', 'type', 'url', 'rating', 'rating_count', 'cover', 'book_id') values('?', '?', '?', '?', '?', '?', '?', '?')
```

æ¢äº†ä¸¤ç§æ–¹å¼ï¼š

``` python
# occupy = str(tuple([0]*len(keys))).replace('0', '?')
occupy = "("+("?,"*len(keys))[:-1]+")"
```

ç¬¬ä¸€ç§å¤ªå•°å—¦åˆå†™äº†ä¸€ä¸ªã€‚ :-!

### 5. å¤šæŠ“é±¼å†é‡è§ 

#### 5.1 ç™»å½• ï¼ˆä½¿ç”¨cookieï¼‰

`requests`ä½¿ç”¨`Cookie`å¾ˆæ–¹ä¾¿ï¼Œç›´æ¥å°†`Cookie`ä»æµè§ˆå™¨å¤åˆ¶è¿‡æ¥ï¼š

```python
url = "https://www.duozhuayu.com/api/user"
headers = {
    'Cookie': '_ga=GA1.2.1145311583.111; _gid=GA1.2.1993378194.111; fish_c0="2|1:0|10:1234567|7:fish_c0|24:N123543553544554545=|xxxxxxxxxxxxxxxxxxxxxxxx"',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}
resp = requests.get(url, headers=headers)
print(json.loads(resp.text))
```
æˆåŠŸï¼Œå“åº”æ•°æ®ä¸ºç”¨æˆ·ä¿¡æ¯ï¼š
```
{'featuresEnabled': {'guess_you_like_test_b': False, 'openCollectionContributionComment': True, 'openCollection': True, 'guess_you_like_test_a': False}, 'isWechatSubscribed': True, 'name': 'åç§°x', 'isPending': False, 'gender': 'unknown', 'avatarSmall': 'https://img.duozhuayu.com/...
```
ä¸å¸¦`Cookie`åˆ™ä¼šå“åº”401ï¼š
```
{'error': {'message': 'ç™»å½•å·²å¤±æ•ˆï¼Œè¯·é‡æ–°ç™»å½•', 'name': 'AUTH_FAILED'}}
```

#### 5.2 æŠ“å–ä¸€ä¸ªåˆ†ç±»ä¸‹æ•°æ® ï¼ˆç”Ÿæˆtokenï¼‰

æ•°æ®æ¥å£åœ¨

`https://www.duozhuayu.com/api/categories/135481276860730989/items? `

ä¸‹ä¸€é¡µä¸º `items?_offset=10&limit=10&after_id=5490 `ï¼Œä¸è¿‡ä¸éœ€è¦è‡ªå·±ç»„è£…ï¼Œåœ¨ç¬¬ä¸€é¡µçš„å“åº”æ•°æ®é‡Œæœ‰ä¸‹ä¸€é¡µçš„é“¾æ¥ã€‚ä½†æ˜¯ï¼Œè¿™äº›éƒ½æ˜¯æ¬¡è¦çš„ï¼Œå› ä¸ºè¯·æ±‚å¤´ä¸­å¸¦æœ‰tokenå‚æ•°ï¼Œåªå¯ä»¥ç”¨æ¥è·å–æ•°æ®ä¸€æ¬¡ã€‚

```
 # åˆ†ç±»æ•°æ®è·å–
    headers={
        'x-api-version': '0.0.5',
        'x-refer-request-id': '0-1546048074037-61023',
        'x-request-id':'0-1546048074346-17714',
        'x-request-misc':'{"platform":"browser"}',
        'x-request-page':'/categories/135481276860730989',
        'x-request-token':'6461338df01a9dad0d4d9e2d09c7e3b4e0f2c5c00a4d0923',
        'x-security-key':'94587384',
        'x-timestamp':'1546048074347',
        'x-user-id':'0'
    }
    y.get_page_data(headers)
```

ç¬¬ä¸€æ¬¡å¯ä»¥è·å–æ•°æ®ï¼Œç¬¬äºŒæ¬¡å°±ä¼šæŠ¥éæ³•è¯·æ±‚ã€‚

##### 5.2.1 è·å–Tokenå’ŒSecurity-key

åˆ†æä¸‹å‚æ•°

```
x-api-version: 0.0.5
x-refer-request-id: 0-1546048074037-61023
x-request-id: 0-1546048074346-17714 # æœªç™»å½•ç¬¬ä¸€ä½ä¸º0ï¼Œå¦åˆ™ä¸º user id
x-request-misc: {"platform":"browser"}
x-request-page: /categories/135481276860730989
x-request-token: 6461338df01a9dad0d4d9e2d09c7e3b4e0f2c5c00a4d0923
x-security-key: 94587384
x-timestamp: 1546048074347
x-user-id: 0  # æœªç™»å½•ä¸º0ï¼Œå¦åˆ™ä¸º user id
```

æœ‰ä¸€äº›æ˜¯å›ºå®šçš„ï¼Œå…¶ä½™çš„éœ€è¦ç”Ÿæˆï¼š

```
x-refer-request-id: 0-1546048074037-61023
x-request-id: 0-1546048074346-17714
x-request-token: 6461338df01a9dad0d4d9e2d09c7e3b4e0f2c5c00a4d0923
x-security-key: 94587384
x-timestamp: 1546048074347
```

`x-refer-request-id` `x-request-id` `x-timestamp` éƒ½æ˜¯å’Œæ—¶é—´æˆ³æœ‰å…³æˆ–è€…è¡ç”Ÿçš„å‚æ•°ï¼Œæ ¸å¿ƒæ˜¯`x-request-token`

å’Œ `x-security-key`ã€‚

æŠ“åŒ…å‘ç°ï¼Œhttps://www.duozhuayu.com/categories/135481276860730989 é¡µé¢åŠ è½½äº†4ä¸ªJSï¼Œæ ¸å¿ƒåªæœ‰1ä¸ª`client.xxxxx.js`ï¼š

```
https://www.google-analytics.com/analytics.js  # ç»Ÿè®¡åˆ†æï¼Œè·³è¿‡
https://static.duozhuayu.com/static/manifest.93ae56791be488b2ba71.js  # ç¼“å­˜è·³è¿‡
https://static.duozhuayu.com/static/common.7ba0706773d1a80cd55e.js # é¡µé¢å…¶ä»–æ“ä½œJS
https://static.duozhuayu.com/static/client.47ba39b15fabaca10208.js # éªŒè¯
```
`client.47ba39b15fabaca10208.js`ä¸­æœ‰æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰å‚æ•°ï¼š
```javascript
var ee = Date.now(), te = Math.floor(1e8 * Math.random()), ne = Object(u.d)(n().session).id || 0, re = s()(ee, ne, te);
return R.headers = Object.assign({}, R.headers, {
    "x-timestamp": ee,
    "x-security-key": te,
    "x-request-token": re,
    "x-user-id": ne
}),
```

éœ€è¦æ‹¿åˆ°ä¸Šé¢çš„å››ä¸ªå‚æ•°ï¼Œ`ne`æ˜¯ç”¨æˆ·id, ä¸º0å³å¯ã€‚è°ƒè¯•JSï¼Œç”¨`Chrome` å’Œ`Firefox`éƒ½å¾ˆå¡ï¼Œæ¢`Edge`å€’æ„å¤–çš„å¥½ã€‚

å¯èƒ½æ˜¯æˆ‘JSç”¨çš„å¤ªçƒ‚ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰æ›´å¥½çš„è°ƒè¯•æ–¹æ³•äº†ï¼Œå¼€ä¸¤ä¸ªé¡µé¢ï¼Œä¸€ä¸ªæ˜¯çœŸå®å¤šæŠ“é±¼é¡µé¢æ–­ç‚¹ï¼Œä¸€ä¸ªæ˜¯æå–å‡ºçš„æ–¹æ³•ï¼ŒåŸé¡µé¢æ‰¾å˜é‡çš„å€¼ï¼Œç„¶åæ”¾åˆ°ä»£ç é‡Œã€‚ç”¨äº†ä¸¤ä¸ªå°æ—¶ç»ˆäºè·å–åˆ°äº†`x-timestamp`, `x-security-key` å’Œ `x-request-token`ï¼Œå¹¶è¯·æ±‚äº†ä¸€æ¬¡ï¼ŒæˆåŠŸã€‚(ä»£ç ä¸­çœå»äº†ä¸€äº›å˜é‡ï¼Œè¯¦ç»†å¯çœ‹Gtihub)

```js
function yH(e,t,n,r,a){r==bG&&a==bG||(e=e[kG]?e[kG](r,a):Array[EG][kG][wG](e,r,a)),t[hG](e,n)}

function mH(e){
    return new Uint8Array(e)
}

function hH(e) {
    for (var t = [], n = IG; n < e[TG]; n += Yz)
        t[pG](e[n] << MU | e[n + rG] << uG | e[n + dG] << Kz | e[n + $z]);
    return t;
}

function zH(t) {
    if (! (this instanceof zH)) throw Error(te);
    Object.defineProperty(this, $, { value: pH(t, SG) }), this[X]();
}

zH[EG][X] = function () {
    var e = 10; //OH[this[$][TG]];
    if (e == bG)
        throw new Error(J);
    this[Z] = [], this[Q] = [];
    for (var t = IG; t <= e; t++)
        this[Z][pG]([IG, IG, IG, IG]), this[Q][pG]([IG, IG, IG, IG]);
    for (var n, r = (e + rG) * Yz, a = this[$][TG] / Yz, o = hH(this[$]), t = IG; t < a; t++)
        n = t >> dG, this[Z][n][t % Yz] = o[t], this[Q][e - n][t % Yz] = o[t];
    for (var i, c = IG, l = a; l < r;) {
        if (i = o[a - rG], o[IG] ^= CH[i >> uG & PG] << MU ^ CH[i >> Kz & PG] << uG ^ CH[i & PG] << Kz ^ CH[i >> MU & PG] ^ jH[c] << MU, c += rG, a != Kz)
            for (var t = rG; t < a; t++)
                o[t] ^= o[t - rG];
        else {
            for (var t = rG; t < a / dG; t++)
                o[t] ^= o[t - rG];
            i = o[a / dG - rG], o[a / dG] ^= CH[i & PG] ^ CH[i >> Kz & PG] << Kz ^ CH[i >> uG & PG] << uG ^ CH[i >> MU & PG] << MU;
            for (var t = a / dG + rG; t < a; t++)
                o[t] ^= o[t - rG];
        }
        for (var s, u, t = IG; t < a && l < r;)
            s = l >> dG, u = l % Yz, this[Z][s][u] = o[t], this[Q][e - s][u] = o[t++], l++;
    }
    for (var s = rG; s < e; s++)
        for (var u = IG; u < Yz; u++)
            i = this[Q][s][u], this[Q][s][u] = FH[i >> MU & PG] ^ DH[i >> uG & PG] ^ MH[i >> Kz & PG] ^ UH[i & PG];
}
zH[EG][Y] = function (e) {
    if (e[TG] != uG)
        throw new Error(q);
    for (var t = this[Z][TG] - rG, n = [IG, IG, IG, IG], r = hH(e), a = IG; a < Yz; a++)
        r[a] ^= this[Z][IG][a];
    for (var o = rG; o < t; o++) {
        for (var a = IG; a < Yz; a++)
            n[a] = PH[r[a] >> MU & PG] ^ IH[r[(a + rG) % Yz] >> uG & PG] ^ NH[r[(a + dG) % Yz] >> Kz & PG] ^ TH[r[(a + $z) % Yz] & PG] ^ this[Z][o][a];
        r = n[kG]();
    }
    for (var i, c = mH(uG), a = IG; a < Yz; a++)
        i = this[Z][t][a], c[Yz * a] = (CH[r[a] >> MU & PG] ^ i >> MU) & PG, c[Yz * a + rG] = (CH[r[(a + rG) % Yz] >> uG & PG] ^ i >> uG) & PG, c[Yz * a + dG] = (CH[r[(a + dG) % Yz] >> Kz & PG] ^ i >> Kz) & PG, c[Yz * a + $z] = (CH[r[(a + $z) % Yz] & PG] ^ i) & PG;
    return c;
}

var WH = function e(t, n, r) {
    if (!(this instanceof e))
        throw Error(te);
    if (this[W] = L, this[OG] = R, n) {
        if (n[TG] != uG)
            throw new Error(T);
    }
    else
        n = mH(uG);
    r || (r = rG), this[N] = r, this[I] = pH(n, SG), this[z] = new zH(t);
};

WH[EG][Y] = function (e) {
    if (e[TG] % this[N] != IG)
        throw new Error(P);
    for (var t, n = pH(e, SG), r = IG; r < n[TG]; r += this[N]) {
        t = this[z][Y](this[I]);
        for (var a = IG; a < this[N]; a++)
            n[r + a] ^= t[a];
        yH(this[I], this[I], IG, this[N]), yH(n, this[I], uG - this[N], r, r + this[N]);
    }
    return n;
}

function dH(e){return parseInt(e)===e}

function fH(e) {
    if (!dH(e[TG]))
        return NG;
    for (var t = IG; t < e[TG]; t++)
        if (!dH(e[t]) || e[t] < IG || e[t] > PG)
            return NG;
    return SG;
}

function pH(e, t) {

    if (e.buffer && ArrayBuffer.isView(e) && e.name === Uint8Array)
        return t && (e = e[kG] ? e[kG]() : Array[EG][kG][wG](e)), e;
    if (Array[xG](e)) {
        if (!fH(e))
            throw new Error("Array contains invalid value:" + e);
        return new Uint8Array(e);
    }
    if (dH(e[TG]) && fH(e))
        return new Uint8Array(e);
    throw new Error("unsupported array-like object");
}

function kH(e) {
    var t = [],
    n = 0;
    for (e = encodeURI(e); n < e.length;) {
        var r = e.charCodeAt(n++);
        r === 37 ? (t.push(parseInt(e.substr(n, 2), 16)), n += 2) : t.push(r)
    }
    return pH(t)
}

function _Ht(e) {
    n = '0123456789abcdef'
    for (var t = [], r = IG; r < e[TG]; r++) {
        var a = e[r];
        t[pG](n[(a & Qz) >> Yz] + n[a & tG]);
    }
    return t[Xz](Jz);
}

function s(e, t, n) {
    var i = [e, t, n].join(':'), c = kH(i), l = new WH(EH, wH), s = l[Y](c);
    return _Ht(s);
}


var ee = Date.now(), 
te = Math.floor(1e8 * Math.random()),
ne = 0;
re = s(ee, ne, te);

console.log(ee)
console.log(te)
console.log(ne)
console.log(re)
```

æ¥ä¸‹æ¥å°è£…JSï¼šä½¿ç”¨`execjs`åº“æ¥å¤„ç†JS

```python
import execjs
import os
import logging
logging.basicConfig(level=logging.DEBUG)
path1 = os.path.dirname(__file__)  # å½“å‰è·¯å¾„


"""
    é€šè¿‡è°ƒç”¨js, è·å–tokenå’ŒåŠ å¯†æ•°æ®
"""

def get_js(path, encodes='utf-8'):
    logging.debug("js file path is "+path)

    f = open(path, 'r', encoding=encodes)  # æ‰“å¼€JSæ–‡ä»¶
    line = f.readline()
    html_str = ''
    while line:
        html_str = html_str + line
        line = f.readline()
    return html_str

def load_sign_js(js_str):
    return execjs.compile(js_str)

def get_token():
    result = {}
    # get token
    sign_js_path = r'' + path1 + "\client.js"
    logging.info('get token js start.')
    sign_js = load_sign_js(get_js(sign_js_path, 'UTF-8'))
    logging.info('get token js ok.')
    result = sign_js.call('get_headers')

    return result
```

```python
## è·å–token
x_data = get_token()
x_data = {k: str(v) for k, v in x_data.items()}

headers={
    'x-api-version': '0.0.5',
    'x-refer-request-id': '0-1546336702230-61023',
    'x-request-id': '0-1546336702230-17714',
    'x-request-misc': '{"platform":"browser"}',
    'x-request-page': '/categories/135481276860730989',
    'x-request-token': x_data['token'],
    'x-security-key': x_data['security'],
    'x-timestamp': x_data['time'],
    'x-user-id': x_data['uid']
}
get_page_data(headers)
```

è·å–æˆåŠŸï¼bingo!

##### 5.2.2 æ”¹è¿›ï¼šç”Ÿæˆå™¨å’ŒSend

ä½¿ç”¨ç”Ÿæˆå™¨å’Œ`send`åŠ¨æ€çš„è·å–`URL`

```python
    def get_page_data(self):
        """ è·å–åˆ†é¡µæ•°æ® """
        next_url = None

        while True:
            logging.debug(f"next_url:{next_url}")
            url = yield next_url
            if not url:
                return
            logging.debug(f"url:{url}")

            headers = self.assem_headers()
            logging.debug(f"headers:{headers}")
            for k, v in headers.items():
                self.headers[k] = v

            data = self.get(url)

            if data:
                logging.info(f"{url} data:{len(data['data'])}, books title:{self.get_all_title(data)}")
                try:
                    next_url = data['paging']['next']
                except KeyError:
                    next_url = None

    def get_data(self, pages, url_primer):
        """ è·å–pagesé¡µæ•°çš„æ•°æ® """
        url_generator = self.get_page_data()
        url_generator.send(None)
        url = url_primer
        for _ in range(pages):
            if not url:
                break
            url = url_generator.send(url)
```

è°ƒç”¨å‡ æ¬¡ä¹‹åï¼ŒæŠ¥é”™`ä»Šæ—¥çš„è®¿é—®é¢åº¦å·²ç”¨å®Œï¼Œè¯·åœ¨å¾®ä¿¡å†…ç»§ç»­ä½¿ç”¨`ã€‚æ¢IPè¯•è¯•

##### 5.2.3 æ·»åŠ ä»£ç†IP

å‚è€ƒ[åˆ©ç”¨Pythonä½¿ç”¨ä»£ç†IP](https://blog.csdn.net/madonghyu/article/details/80645406)ä¸­çš„ï¼Œå»`https://www.xicidaili.com/`å»æ‹¿IPã€‚ä¸è¿‡ä½œè€…è¿™ç§å†™æ³•å•°å—¦äº†ï¼Œç”¨`BeautifulSoup`åˆ†æäº†é¡µé¢åè¿˜ç”¨æ­£åˆ™ç”¨çš„æ¯”è¾ƒå•°å—¦ã€‚ç›´æ¥æ”¹ä¸€å¥è¯æ­£åˆ™ï¼Œè¿™æ‰æ˜¯æ­£åˆ™ç²—æš´çš„å¼ºå¤§ã€‚

åŸæ–‡ä¸­è·å–ä»£ç†IPåˆ—è¡¨æ–¹æ³•ï¼š

```python
def get_proxy_list():
            target = 'http://www.xicidaili.com/nn/' + str(random.randint(0, 100))
            try:
                opener = urllib.request.build_opener()
                #headerå¯ä»¥é€‰è‡ªå·±æµè§ˆå™¨çš„
                #æ ·ä¾‹ï¼š[('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) ''AppleWebKit/537.36 (KHTML, like Gecko)Chrome/56.0.2924.87 Safari/537.36')]
                opener.addheaders = self.headers
                urllib.request.install_opener(opener)
                html = urllib.request.urlopen(target).read().decode('utf-8')
                tr = BeautifulSoup(html, 'lxml').find_all('tr')
                p = re.compile('<[^>]+>')
                for tag in tr:
                    td_list = tag.find_all('td')
                    if len(td_list) > 0:
                        if str(td_list[5]) == '<td>HTTP</td>':
                        #å°†çˆ¬åˆ°çš„ä»£ç†IPå­˜åˆ°åˆ—è¡¨é‡Œé¢
                            self.proxy_list.append(p.sub('', str(td_list[1])) + ':' + p.sub('', str(td_list[2])))
            except Exception as b:
                self.logger.exception(b)

```

æ­£åˆ™æ”¹åï¼š

```python
# coding:utf-8
""" ä»£ç†IP """
import requests
from random import randint
import re


class Proxy(object):
    def __init__(self):
        self.target = f"https://www.xicidaili.com/wn/{randint(0, 100)}"
        self.headers = {'User-Agent': "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11"}

    def get(self):
        resp = requests.get(self.target, headers=self.headers)
        data = resp.text
        ip_list = re.findall(r'((\d{1,3}\.){3}\d+)[^\d]+(\d+)', data)
        proxy_list = [f"{i[0]}:{i[2]}" for i in ip_list]
        return proxy_list

if __name__ == "__main__":
    data = Proxy().get()
    print(data)
```

è¾“å‡ºï¼š

```
['121.61.25.213:9999', '121.61.33.80:9999', ...]
```

æ­£åˆ™å¯è§†åŒ–ï¼š

![proxy.png](http://image.acfuu.com/mdImages/201901/proxy.png)

ç®€å•çš„ä»£ç†æ± å¥½äº†ï¼Œä»£ç†ä¸ºhttps

```python
self.proxies = {
	'https': "https://" + self.proxy_ip
}
```

ç”¨äº†å¥½ä¹…å»è®¿é—®ï¼Œä¸€ç›´æŠ¥`ç”±äºç›®æ ‡è®¡ç®—æœºç§¯ææ‹’ç»ï¼Œæ— æ³•è¿æ¥`ï¼Œä»¥ä¸ºæ˜¯ä»£ç†å’Œä½†æ˜¯`requests.get`æ—¶å‚æ•°ä¸º`verify=False`çš„åŸå› å‘¢ï¼Œå¼„äº†å¥½ä¹…ä¸æ˜¯çš„ã€‚åªæ˜¯å› ä¸ºä»£ç†å…¨æŒ‚äº†ï¼ŒåŸæ¥ä¸æ˜¯ä»£ç†æ± è€Œæ˜¯ä¸€æ½­æ­»æ°´å•Šã€‚

å†™ä¸€ä¸ª**éªŒè¯ä»£ç†ipæ˜¯å¦æœ‰æ•ˆ**çš„æ–¹æ³•ï¼š

```python
def check_proxy_ip(self):
    """ éªŒè¯IPæ˜¯å¦å¯ç”¨ """
    if not self.proxy_ip:
        self.proxy_ip = self.get_proxy()
    if self.proxy_ip:
        ip, port = self.proxy_ip.split(':')
        logging.debug(f"check ip {ip}:{port}")
        try:
            telnetlib.Telnet(ip, port, timeout=10)
        except Exception as e:
            logging.debug('error')
            self.proxy_ip = None
            return False
            # self.check_proxy_ip()
        else:
            return True
```

éªŒè¯äº†å¥½å¤šé¡µï¼Œå‘ç°æ²¡æœ‰ä¸€ä¸ªå¥½çš„ï¼Œè¿™ä¸ªç«™ä¹Ÿå¤ªæ°´äº†ã€‚åœ¨æŸç«™æ‰‹åŠ¨é€‰äº†å‡ ä¸ªå‘ç°ä¸€ä¸ªå¥½çš„ :D

```
https://61.145.182.27:53281
```

ä¼˜åŒ–ä»£ç åæ‹¿ä¸Šå»è¯•ï¼Œæ‹¿åˆ°æœ‰æ•ˆæ•°æ®ï¼ŒéªŒè¯äº†ç½‘ç«™åªæ˜¯å°æ€äº†IPã€‚

```
INFO:root:https://www.duozhuayu.com/api/categories/135481276860730989/items?limit=15&after_id=2014935363555&offset=30 data:15, books title:['ç¿çƒ‚åƒé˜³', 'çœ‹è§', 'æ¸´æœ›ç”Ÿæ´»', 'é˜…è¯»æ˜¯ä¸€åº§éšèº«æºå¸¦çš„é¿éš¾æ‰€', 'å¾®ç§¯åˆ†å­¦æ•™ç¨‹ï¼ˆç¬¬3å·ï¼‰', 'é›•æ¢ç”»æ ‹', 'æœˆäº®ä¸å…­ä¾¿å£«', 'ä¸‡æ°´åƒå±±èµ°é', 'å…¬ä¸»èµ°è¿›é»‘æ£®æ—ï¼šç”¨è£æ ¼çš„è§‚ç‚¹æ¢ç´¢ç«¥è¯ä¸–ç•Œ', 'æ¶æ„', 'å°¤æ¯”å…‹', 'å¤©ç”Ÿå¹¸å­˜è€…', 'è”é‚¦å…šäººæ–‡é›†', 'å½±å“åŠ›', 'æ¼¢èªä¿—å­—ç ”ç©¶ï¼ˆå¢è¨‚æœ¬ï¼‰']
DEBUG:root:next_url:https://www.duozhuayu.com/api/categories/135481276860730989/items?limit=15&after_id=2014925696244&offset=45
```

åˆ°è¿™é‡Œå°±å°†å¤šæŠ“é±¼çš„åˆ†ç±»èƒ½æ‹¿ä¸‹æ¥äº†ï¼Œç»å†äº†è§£ææ•°æ®ã€è·å–Tokenã€ä½¿ç”¨`yield`å’Œ`send`ã€ä½¿ç”¨å’Œæ£€æµ‹ä»£ç†IPã€‚



### 6. Scrapy ä½¿ç”¨

[å®˜æ–¹æ–‡æ¡£å¾ˆç®€å•](https://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html)

å°±æ¥è¯•ä¸€ä¸ªçˆ¬å–æ‰€æœ‰[è…¾è®¯å…¨ç«¯ AlloyTeam å›¢é˜Ÿåšå®¢](http://www.alloyteam.com/page/0/)æ–‡ç« çš„å°é¡¹ç›®å§ã€‚

æ–‡ç« è¯¦æƒ…æ²¡æœ‰æ¥å£ï¼Œç›´æ¥æ¸²æŸ“çš„é¡µé¢è¾“å‡ºã€‚ä¿å­˜åšå®¢å†…å®¹æœ‰ä¸¤ç§å–èˆï¼Œ1. å­˜å‚¨åŒ…å«`div`ä¾¿ç­¾çš„å†…å®¹ 2. åªå–æ–‡æœ¬ã€‚è¿™ä¸¤ç§éƒ½ä¸å¦¥ï¼Œä¸€ä¸ªæ–‡ç« ä¸èƒ½ä¸¢äº†é“¾æ¥å’Œå›¾ç‰‡ï¼Œè€Œä¸”æŠ€æœ¯æ–‡ç« å¿…æœ‰`code`å—ã€‚çŒœæµ‹åšæ–‡å†…å®¹ä½¿ç”¨çš„`markdown`ï¼Œæ˜¯ä¸æ˜¯mdä¹Ÿä¸é‡è¦ï¼Œæ˜ç¡®äº†è‡ªå·±çš„éœ€æ±‚ï¼šçˆ¬å–åšå®¢å†…å®¹ï¼Œå¿…é¡»æœ‰é“¾æ¥å’Œå›¾ç‰‡ä»¥åŠä»£ç å—ï¼Œç”¨`markdown`å­˜å‚¨ã€‚

#### 6.1 åˆ†æè§£æè¿‡ç¨‹æ‹¿åˆ°å†…å®¹

BeautifulSoupä¸­æ ‡ç­¾æœ‰`next_element `å±æ€§ï¼Œå¯ä»¥æŒ‡å‘ä¸‹ä¸€ä¸ªè§£æçš„å¯¹è±¡ã€‚ç”¨`<p><a>åŸæ–‡åœ°å€</a></p>`æ ‡ç­¾ä¸ºä¾‹ï¼Œè¾“å‡ºæ¯ä¸ªæ ‡ç­¾`content`å’Œç›¸åº”çš„æ ‡ç­¾å`tag`:

```python
while True:
	try:
		print(f"content:{p}  tag:{p.name}")
		p = p.next_element
	except AttributeError as e:
		print(e)
		break
```

![bs4_next_element.png](http://image.acfuu.com/mdImages/201901/bs4_next_element.png)

é€šè¿‡ç»“æœå¯ä»¥çœ‹åˆ°é€šè¿‡æ ‡ç­¾ä¾æ¬¡è¯»å–ï¼ˆå°±æ˜¯bs4çš„è§£æè¿‡ç¨‹ï¼‰ï¼š `p > a > å†…å®¹`
```
content:<p><a href="##">åŸæ–‡åœ°å€</a></p>  tag:p
content:<a href="##">åŸæ–‡åœ°å€</a>  tag:a
content:åŸæ–‡åœ°å€  tag:None
```

é€šè¿‡`next_element `å¯è·å–åˆ°æ‰€æœ‰çš„æ ‡ç­¾ï¼Œå½“æ ‡ç­¾åä¸º`None`æ—¶ï¼Œåˆ™ä»£è¡¨ä¸ºå†…å®¹ï¼Œä¸ä¸º`None`æ—¶ï¼Œåˆ†åˆ«åˆ†æã€‚

#### 6.2 å¤„ç†æ‰€éœ€æ•°æ®

åˆ†åˆ«å¤„ç†`a`,`img`ä»¥åŠä»£è¡¨ä»£ç å—çš„æ ‡ç­¾`div.crayon-pre`åˆ†åˆ«å¤„ç†ã€‚

![allow_crayon_ore.png](http://image.acfuu.com/mdImages/201901/allow_crayon_ore.png)

ä»£ç å—ä¸èƒ½ç›´æ¥å–`div.crayon-pre`ç„¶å`get_text()`ï¼Œç»“æœ` ./imgâ”œâ”€â”€ gka_1.pngâ”œâ”€â”€ gka_2.pngâ”œâ”€â”€ gka_3.pngâ””â”€â”€ ...`æ²¡æœ‰æ¢è¡Œã€‚æ‰€ä»¥å¾—è½¬æ¢ä¸€ä¸‹ï¼š

```python
elif tag_name == 'div' and tag.has_attr('class') and _code_class in tag['class']:
    logging.debug("ä»£ç å—")
    # è·å–æ¯è¡Œcodeï¼Œæ·»åŠ å›è½¦
    temp_code = []
    for line in tag.select('div.crayon-line')[:]:
        line_code = line.get_text()
        temp_code.append(line_code)
    code = '\n'.join(temp_code)
    logging.debug(f"code={code}")
    # é¿å…å†…å®¹å†å–åˆ°ï¼Œå–å‰ä¸€ä¸ªå¯¹è±¡çš„å…„å¼Ÿæ ‡ç­¾
    tag = tag.previous_element.next_sibling
    continue
```

è¾“å‡ºï¼š

```
DEBUG:root:ä»£ç å—
DEBUG:root:code=Â 
./img
â”œâ”€â”€ gka_1.png
â”œâ”€â”€ gka_2.png
â”œâ”€â”€ gka_3.png
â””â”€â”€ ...
```

å› ä¸ºä¸€äº›åŸå› æœ€åä½¿ç”¨äº†`crayon-row`ã€‚

ç”±äº`next_element `ä¼šä¸€ç›´è¿­ä»£åˆ°åŸæ–‡æ¡£çš„æœ€åï¼Œæ‰€ä»¥å°†å†…å®¹æ–‡æ¡£æ ‘ç‹¬ç«‹å‡ºæ¥ï¼Œå¯ä»¥ä½¿ç”¨`extract()`æ–¹æ³•ï¼š

> è¿™ä¸ªæ–¹æ³•å®é™…ä¸Šäº§ç”Ÿäº†2ä¸ªæ–‡æ¡£æ ‘: ä¸€ä¸ªæ˜¯ç”¨æ¥è§£æåŸå§‹æ–‡æ¡£çš„Â `BeautifulSoup`Â å¯¹è±¡,å¦ä¸€ä¸ªæ˜¯è¢«ç§»é™¤å¹¶ä¸”è¿”å›çš„tag.è¢«ç§»é™¤å¹¶è¿”å›çš„tagå¯ä»¥ç»§ç»­è°ƒç”¨Â `extract`æ–¹æ³•ã€‚

è¿‡äº†äº”ç™¾å¹´... ä¸­é€”åŠ äº†æ ‡é¢˜çš„è·å–ï¼Œå› ä¸ºå¾ˆç®€å•

```python
elif re.search(r'h\d', tag_name):
logging.debug("æ ‡é¢˜")
head_grade = int(re.sub(r"h(\d)", r"\1",tag_name))
md_head = f"{'#'*head_grade} {tag.string}"
content_text.append(md_head)
tag = tag.next_element
```

æœ€ç»ˆçš„ ~~htmlè½¬mdçš„~~ ç®€é™‹æ•ˆæœï¼š

![htmltomd.gif](http://image.acfuu.com/mdImages/201901/htmltomd.gif)

`Scrapy`çœŸæ˜¯ä¸ªå¥½ä¸œè¥¿ï¼Œè®©æˆ‘æ²¡å…³å¿ƒçˆ¬è™«ï¼ŒæŠŠç²¾åŠ›éƒ½æ”¾åˆ°è·å–å†…å®¹ä¸Šã€‚éš¾é“è¿™å°±æ˜¯æœ¬èŠ‚æ²¡æœ‰æœºä¼šè°ˆ`Scrapy`è€Œä¸€ç›´åœ¨å¼„`BeautifulSoup`çš„åŸå›  :)

å°†æŠ“å–å†…å®¹è¿™å—é›†æˆåˆ°çˆ¬è™«é‡Œé¢å¹¶ä¸”è¾“å‡ºå°±å¥½äº†ï¼Œç»“æŸã€‚çº³å°¼ï¼Ÿå®Œå…¨æ²¡æœ‰è¯´åˆ°ä½¿ç”¨`Scrapy`å˜›ã€‚

#### 6.3 ä½¿ç”¨è¿‡ç¨‹

ç®€è¿°ï¼šç®€å•å®ç”¨ä¸”é«˜æ•ˆï¼Œå°±ä½¿ç”¨äº†ä¸‰ä¸ªå‘½ä»¤

>  åˆ›å»ºé¡¹ç›®`scrapy startproject project_name`

>  çˆ¬å–æ“ä½œ`scrapy crawl spider_name`

>  è¾“å‡ºæ–‡ä»¶`scrapy crawl spider_name -o xxx.json`

æ“ä½œï¼š

åˆ›å»ºé¡¹ç›®ä½¿ç”¨å‘½ä»¤ `scrapy startproject allow_blog`

æ•°æ®å®¹å™¨ï¼š

```python
# file: items.py
import scrapy

class AlloyBlogItem(scrapy.Item):
    url = scrapy.Field()
    urid = scrapy.Field()
    title = scrapy.Field()
    author = scrapy.Field()
    content = scrapy.Field()
```

çˆ¬è™«æ–‡ä»¶å†…å®¹ç²¾ç®€å¦‚ä¸‹ï¼š

```python
# file: Spider/blog.py
class BlogSpider(Spider):
    name = "blog"
    allowd_domains = ["alloyteam.com"]
    start_urls = ['http://www.alloyteam.com/page/0/']

    def parse(self, response):
        """ è§£æå‡½æ•° """
        urls = re.findall(r'href="([^\"]*)".*blogTitle', response.text)
        for url in urls:
            yield Request(url, callback=self.parse_article)

    def parse_article(self, response):
        """ è§£æåšå®¢æ–‡ç«  """
        url = response.url

        soup = BeautifulSoup(response.text, 'lxml')

        item = AlloyBlogItem()
        item['title'] = soup.find('a', 'blogTitle btitle').string
        item['author'] = soup.find(rel='author').string
        item['url'] = url
        item['urid'] = url.split('/')[-2]

        # å†…å®¹è·å–
        content = soup.select(".content_banner > .text")[0]
        md_text = self.html_to_md(content)
        md_str = '\n'.join(md_text)
        md_str = f"> [{item['title']}]({url})\n" + md_str
        item['content'] = md_str

        '''
        # ä¹Ÿå¯ç›´æ¥å†™å…¥mdåç¼€æ–‡ä»¶
        path = item['urid'] + '.md'
        with open(path, 'w', encoding='utf-8') as f:
            f.write(path)
        '''

        yield item

    @staticmethod
    def html_to_md(content_tag):
        """ æ–‡ç« å†…å®¹htmlè§£æä¸ºmd """
        pass
```

è¿è¡Œå‘½ä»¤`scrapy crawl blog`å³å¼€å§‹è‡ªåŠ¨çˆ¬å–ï¼Œè¿™é‡Œçš„`blog`ä¸ºçˆ¬è™«åï¼Œæ˜¯ä¸Šé¢`BlogSpider`ä¸­çš„`name="blog"`å±æ€§ã€‚

ä½†è¿™é‡Œè¿˜æœªå­˜å‚¨ï¼Œå¯å¼€å¯æ³¨é‡Šä¸­çš„ç›´æ¥å†™å…¥`md`åç¼€çš„`markdown`æ–‡ä»¶ï¼Œåç§°ä¼šå–`url`ä¸­çš„è·¯å¾„åå½“å”¯ä¸€idã€‚æˆ–è€…ä½¿ç”¨å‘½ä»¤`scrapy crawl blog -o data.json`å­˜å‚¨è‡ªå®šä¹‰çš„`AlloyBlogItem`æ•°æ®ï¼ˆç±»ä¼¼å­—å…¸æ•°æ®ï¼‰ä¸º`json`æ–‡ä»¶ã€‚

æ•°æ®å±•ç¤ºï¼š10ä¸ª`md`æ–‡ä»¶ä»¥åŠä¸€ä¸ª`json`æ–‡ä»¶ï¼š

![scrapy_data.png](http://image.acfuu.com/mdImages/201901/scrapy_data.png)

`json`æ–‡ä»¶ä¹Ÿæœ‰åæ¡æ•°æ®ï¼Œæ¯æ¡éƒ½åŒ…å«`title` `author` `url` `urid` `content`ï¼Œä¸Šé¢å®šä¹‰çš„`AlloyBlogItem`æ•°æ®ã€‚

![scrapy_data_json.png](http://image.acfuu.com/mdImages/201901/scrapy_data_json.png)

å…¶ä»–æ–‡ä»¶ï¼šç®¡é“`pipelines.py`å¤„ç†æ•°æ®ã€ä¸­é—´ä»¶`middlewares.py`å’Œè®¾ç½®`settings .py`ä»¥åå†ç”¨å§ã€‚

çˆ¬è™«å°±åˆ°è¿™å„¿å§ã€‚è¿™ç¯‡æ–­æ–­ç»­ç»­å®è·µæœ‰åŠä¸ªæœˆäº†ï¼Œ19å¹´çš„ç¬¬ä¸€ç¯‡æŠ€æœ¯æ–‡ç« å°±æ‹¿è¿™ç¯‡æ»¥ç«½å……æ•°å§ï¼

### å‚è€ƒ

- [Pythonçˆ¬è™«ç³»åˆ—-ä¿å¤](https://www.yukunweb.com/2017/5/python-spider-basic/)
- [Requests: è®© HTTP æœåŠ¡äººç±»](http://docs.python-requests.org/zh_CN/latest/index.html)
- [Beautiful Soup 4.2.0 æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)
- [Scrapy 1.0 æ–‡æ¡£](https://scrapy-chs.readthedocs.io/zh_CN/1.0/index.html)
- [ã€Pythonã€‘å¯åŠ¨è¿…é›·ä¸‹è½½](https://www.cnblogs.com/zeze/p/8052372.html)
- [Pythonç¬¬ä¸‰æ–¹åº“jiebaï¼ˆä¸­æ–‡åˆ†è¯ï¼‰å…¥é—¨ä¸è¿›é˜¶ï¼ˆå®˜æ–¹æ–‡æ¡£ï¼‰](https://blog.csdn.net/qq_34337272/article/details/79554772)
- [Python3 jiebaåˆ†è¯](https://blog.csdn.net/sinat_34022298/article/details/75943272)
- [python3å®˜æ–¹æ–‡æ¡£ï¼š ç®€å•çš„sqlite shell](https://docs.python.org/3/library/sqlite3.html#sqlite3.complete_statement)
- [Pythonç”Ÿæˆå™¨generatorä¹‹nextå’Œsendè¿è¡Œæµç¨‹](https://blog.csdn.net/pfm685757/article/details/49924099)
- [ä½¿ç”¨pythonéªŒè¯ä»£ç†ipæ˜¯å¦å¯ç”¨](https://www.jianshu.com/p/588241a313e7)

